# é»˜è®¤è§„åˆ’è¯æç¤ºè¯æ¨¡ç‰ˆ
MY_DEFAULT_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„AIè§„åˆ’ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·æå‡ºçš„å¤æ‚é—®é¢˜åˆ†è§£æˆä¸€ä¸ªç”±å¤šä¸ªç®€å•æ­¥éª¤ç»„æˆçš„è¡ŒåŠ¨è®¡åˆ’ã€‚
è¯·ç¡®ä¿è®¡åˆ’ä¸­çš„æ¯ä¸ªæ­¥éª¤éƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ï¼Œå¯æ‰§è¡Œçš„å­ä»»åŠ¡ï¼Œå¹¶ä¸”ä¸¥æ ¼æŒ‰ç…§é€»è¾‘é¡ºåºæ’åˆ—ã€‚
ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªPythonåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªæè¿°å­ä»»åŠ¡çš„è‡ªç„¶ä¸²ã€‚

é—®é¢˜:
{question}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºä½ çš„è®¡åˆ’ï¼š
''' python
["æ­¥éª¤1","æ­¥éª¤2","æ­¥éª¤3",...]
'''
"""

# é»˜è®¤æ‰§è¡Œå™¨æç¤ºè¯æ¨¡ç‰ˆ
DEFAULT_EXECUTOR_PROMPT = """
ä½ æ˜¯ä¸€ä½é¡¶çº§çš„AIæ‰§è¡Œä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸¥æ ¼æŒ‰ç…§ç»™å®šçš„è®¡åˆ’ï¼Œä¸€æ­¥æ­¥åœ°è§£å†³é—®é¢˜ã€‚
ä½ å°†æ¥æ”¶åˆ°åŸå§‹é—®é¢˜ï¼Œå®Œæ•´çš„è®¡åˆ’ï¼Œä»¥åŠåˆ°ç›®å‰ä¸ºæ­¢å·²ç»å®Œæˆçš„æ­¥éª¤å’Œç»“æœã€‚
è¯·ä½ ä¸“æ³¨äºè§£å†³â€œå½“å‰æ­¥éª¤â€œï¼Œå¹¶è¾“å‡ºè¯¥æ­¥éª¤çš„æœ€ç»ˆç­”æ¡ˆï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–å¯¹è¯

# åŸå§‹é—®é¢˜ï¼š
{question}

# å®Œæ•´è®¡åˆ’ï¼š
{plan}

# å†å²æ­¥éª¤ä¸ç»“æœï¼š
{history}

# å½“å‰æ­¥éª¤ï¼š
{current_step}

è¯·ä»…è¾“å‡ºé’ˆå¯¹â€œå½“å‰æ­¥éª¤â€œçš„å›ç­”ï¼š
"""

# å¯¼å…¥å¿…è¦çš„åº“
import ast  # ç”¨äºå®‰å…¨åœ°è¯„ä¼°å­—ç¬¦ä¸²å½¢å¼çš„Pythonå­—é¢é‡ï¼ˆå¦‚åˆ—è¡¨ï¼‰
from typing import Optional, List, Dict, Any  # ç”¨äºç±»å‹æ³¨è§£ï¼Œå¢å¼ºä»£ç å¯è¯»æ€§å’Œå¥å£®æ€§
from hello_agents import HelloAgentsLLM  # å¯¼å…¥è‡ªå®šä¹‰çš„å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯
from agent import Agent  # å¯¼å…¥åŸºç¡€Agentç±»
from messages import Message  # å¯¼å…¥æ¶ˆæ¯ç±»ï¼Œç”¨äºè®°å½•å¯¹è¯å†å²
from config import Config  # å¯¼å…¥é…ç½®ç±»

class Planner:
    """
    è§„åˆ’å™¨ (Planner) - è´Ÿè´£å°†ç”¨æˆ·çš„å¤æ‚é—®é¢˜åˆ†è§£ä¸ºä¸€ç³»åˆ—æ›´ç®€å•ã€å¯æ‰§è¡Œçš„æ­¥éª¤ã€‚
    è¿™æ˜¯å®ç°â€œè§„åˆ’ä¸è§£å†³â€æ¨¡å¼çš„ç¬¬ä¸€æ­¥ã€‚
    """
    def __init__(self, llm_client: HelloAgentsLLM, prompt_template: Optional[str] = None):
        """
        åˆå§‹åŒ–è§„åˆ’å™¨ã€‚

        Args:
            llm_client (HelloAgentsLLM): ç”¨äºä¸å¤§è¯­è¨€æ¨¡å‹äº¤äº’çš„å®¢æˆ·ç«¯å®ä¾‹ã€‚
            prompt_template (Optional[str]): å¯é€‰çš„è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä½¿ç”¨é»˜è®¤æ¨¡æ¿ MY_DEFAULT_PROMPTã€‚
        """
        self.llm_client = llm_client  # ä¿å­˜LLMå®¢æˆ·ç«¯å®ä¾‹
        # å¦‚æœç”¨æˆ·æ²¡æœ‰æä¾›è‡ªå®šä¹‰æ¨¡æ¿ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„è§„åˆ’æç¤ºè¯æ¨¡æ¿
        self.prompt_template = prompt_template if prompt_template else MY_DEFAULT_PROMPT

    def plan(self, question: str, **kwargs) -> List[str]:
        """
        æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç”Ÿæˆä¸€ä¸ªè¡ŒåŠ¨è®¡åˆ’ã€‚

        Args:
            question (str): ç”¨æˆ·æå‡ºçš„éœ€è¦è§£å†³çš„å¤æ‚é—®é¢˜ã€‚
            **kwargs: ä¼ é€’ç»™LLMè°ƒç”¨çš„é¢å¤–å‚æ•° (ä¾‹å¦‚ temperature, max_tokensç­‰)ã€‚

        Returns:
            List[str]: ä¸€ä¸ªåŒ…å«å¤šä¸ªæ­¥éª¤æè¿°å­—ç¬¦ä¸²çš„åˆ—è¡¨ã€‚å¦‚æœç”Ÿæˆæˆ–è§£æå¤±è´¥ï¼Œåˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚
        """
        # å°†ç”¨æˆ·é—®é¢˜å¡«å……åˆ°æç¤ºè¯æ¨¡æ¿ä¸­ï¼Œç”Ÿæˆå®Œæ•´çš„prompt
        prompt = self.prompt_template.format(question=question)
        # æ„é€ ç¬¦åˆLLM APIæ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
        messages = [{"role": "user", "content": prompt}]

        print("--- æ­£åœ¨ç”Ÿæˆè®¡åˆ’ ---")
        # è°ƒç”¨LLMï¼Œè·å–ç”Ÿæˆçš„è®¡åˆ’æ–‡æœ¬ã€‚å¦‚æœè¿”å›Noneï¼Œåˆ™é»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ã€‚
        response_text = self.llm_client.invoke(messages, **kwargs) or ""
        print(f"âœ… è®¡åˆ’å·²ç”Ÿæˆ:\n{response_text}")

        try:
            # å°è¯•ä»LLMçš„å“åº”ä¸­æå–Pythonä»£ç å—é‡Œçš„å†…å®¹
            # å‡è®¾å“åº”æ ¼å¼ä¸º " ... ```python\n['æ­¥éª¤1', 'æ­¥éª¤2']\n``` ... "
            plan_str = response_text.split("```python")[1].split("```")[0].strip()
            # ä½¿ç”¨ ast.literal_eval å®‰å…¨åœ°å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºPythonåˆ—è¡¨å¯¹è±¡ï¼Œé¿å…eval()çš„å®‰å…¨é£é™©
            plan = ast.literal_eval(plan_str)
            # ç¡®ä¿è§£æç»“æœç¡®å®æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå¦åˆ™è¿”å›ç©ºåˆ—è¡¨
            return plan if isinstance(plan, list) else []
        except (ValueError, SyntaxError, IndexError) as e:
            # æ•è·è§£æè¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„é”™è¯¯ï¼ˆå¦‚æ ¼å¼ä¸æ­£ç¡®ï¼Œæ‰¾ä¸åˆ°ä»£ç å—ç­‰ï¼‰
            print(f"âŒ è§£æè®¡åˆ’æ—¶å‡ºé”™: {e}")
            print(f"åŸå§‹å“åº”:{response_text}")
            return []  # è§£æå¤±è´¥æ—¶è¿”å›ç©ºåˆ—è¡¨
        except Exception as e:
            # æ•è·å…¶ä»–æœªçŸ¥å¼‚å¸¸
            print(f"âŒ è§£æè®¡åˆ’æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return []

class Executor:
    """
    æ‰§è¡Œå™¨ (Executor) - è´Ÿè´£æŒ‰ç…§è§„åˆ’å™¨ç”Ÿæˆçš„è®¡åˆ’ï¼Œä¸€æ­¥æ­¥åœ°æ‰§è¡Œä»»åŠ¡ã€‚
    å®ƒä¼šåœ¨æ‰§è¡Œæ¯ä¸€æ­¥æ—¶ï¼Œéƒ½è€ƒè™‘åŸå§‹é—®é¢˜ã€å®Œæ•´è®¡åˆ’ä»¥åŠä¹‹å‰æ­¥éª¤çš„ç»“æœã€‚
    """
    def __init__(self, llm_client: HelloAgentsLLM, prompt_template: Optional[str] = None):
        """
        åˆå§‹åŒ–æ‰§è¡Œå™¨ã€‚

        Args:
            llm_client (HelloAgentsLLM): ç”¨äºä¸å¤§è¯­è¨€æ¨¡å‹äº¤äº’çš„å®¢æˆ·ç«¯å®ä¾‹ã€‚
            prompt_template (Optional[str]): å¯é€‰çš„è‡ªå®šä¹‰æ‰§è¡Œæç¤ºè¯æ¨¡æ¿ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä½¿ç”¨é»˜è®¤æ¨¡æ¿ DEFAULT_EXECUTOR_PROMPTã€‚
        """
        self.llm_client = llm_client  # ä¿å­˜LLMå®¢æˆ·ç«¯å®ä¾‹
        # å¦‚æœç”¨æˆ·æ²¡æœ‰æä¾›è‡ªå®šä¹‰æ¨¡æ¿ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„æ‰§è¡Œæç¤ºè¯æ¨¡æ¿
        self.prompt_template = prompt_template if prompt_template else DEFAULT_EXECUTOR_PROMPT

    def execute(self, question: str, plan: List[str], **kwargs) -> str:
        """
        æŒ‰é¡ºåºæ‰§è¡Œè®¡åˆ’ä¸­çš„æ¯ä¸€ä¸ªæ­¥éª¤ï¼Œå¹¶è¿”å›æœ€ç»ˆç»“æœã€‚

        Args:
            question (str): ç”¨æˆ·çš„åŸå§‹é—®é¢˜ã€‚
            plan (List[str]): ç”±è§„åˆ’å™¨ç”Ÿæˆçš„æ­¥éª¤åˆ—è¡¨ã€‚
            **kwargs: ä¼ é€’ç»™LLMè°ƒç”¨çš„é¢å¤–å‚æ•°ã€‚

        Returns:
            str: æ‰§è¡Œå®Œæ‰€æœ‰æ­¥éª¤åå¾—åˆ°çš„æœ€ç»ˆç­”æ¡ˆã€‚
        """
        history = ""  # ç”¨äºå­˜å‚¨å·²å®Œæˆæ­¥éª¤åŠå…¶ç»“æœçš„å­—ç¬¦ä¸²ï¼Œä½œä¸ºåç»­æ­¥éª¤çš„ä¸Šä¸‹æ–‡
        final_answer = ""  # ç”¨äºå­˜å‚¨æœ€åä¸€ä¸ªæ­¥éª¤çš„è¾“å‡ºä½œä¸ºæœ€ç»ˆç­”æ¡ˆ

        print("\n--- æ­£åœ¨æ‰§è¡Œè®¡åˆ’ ---")
        # éå†è®¡åˆ’ä¸­çš„æ¯ä¸€ä¸ªæ­¥éª¤ï¼Œå¹¶å¸¦ä¸Šç´¢å¼•ï¼ˆä»1å¼€å§‹ï¼‰
        for i, step in enumerate(plan, 1):
            print(f"\n -> æ­£åœ¨æ‰§è¡Œæ­¥éª¤ {i}/{len(plan)}:{step}")
            # å‡†å¤‡å½“å‰æ­¥éª¤çš„æç¤ºè¯ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
            prompt = self.prompt_template.format(
                question=question,
                plan=plan,
                history=history if history else "æ— ",  # å¦‚æœå†å²ä¸ºç©ºï¼Œåˆ™æ˜¾ç¤º"æ— "
                current_step=step
            )
            # æ„é€ LLM APIçš„æ¶ˆæ¯
            messages = [{"role": "user", "content": prompt}]

            # è°ƒç”¨LLMæ‰§è¡Œå½“å‰æ­¥éª¤
            response_text = self.llm_client.invoke(messages, **kwargs) or ""
            # å°†å½“å‰æ­¥éª¤å’Œå…¶ç»“æœè¿½åŠ åˆ°å†å²è®°å½•ä¸­ï¼Œä¸ºä¸‹ä¸€æ­¥æä¾›ä¸Šä¸‹æ–‡
            history += f"æ­¥éª¤{i}:{step}\n ç»“æœ:{response_text}\n\n"
            # æ›´æ–°æœ€ç»ˆç­”æ¡ˆä¸ºå½“å‰æ­¥éª¤çš„ç»“æœï¼ˆå¾ªç¯ç»“æŸåï¼Œè¿™å°†æ˜¯æœ€åä¸€ä¸ªæ­¥éª¤çš„ç»“æœï¼‰
            final_answer = response_text
            print(f"âœ… æ­¥éª¤{i}å·²å®Œæˆï¼Œç»“æœ:{final_answer}")

        # è¿”å›æœ€åä¸€ä¸ªæ­¥éª¤çš„æ‰§è¡Œç»“æœä½œä¸ºæ•´ä¸ªä»»åŠ¡çš„æœ€ç»ˆç­”æ¡ˆ
        return final_answer

class PlanAndSolveAgent(Agent):
    """
    è§„åˆ’ä¸è§£å†³ï¼ˆPlan and Solveï¼‰æ™ºèƒ½ä½“ - é‡‡ç”¨ä¸¤æ­¥èµ°çš„ç­–ç•¥æ¥è§£å†³å¤æ‚é—®é¢˜ã€‚

    è¿™ä¸ªAgentèƒ½å¤Ÿï¼š
    1.  å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªç®€å•æ­¥éª¤ï¼ˆè§„åˆ’é˜¶æ®µï¼‰ã€‚
    2.  æŒ‰ç…§ç”Ÿæˆçš„è®¡åˆ’é€æ­¥æ‰§è¡Œï¼Œæ¯ä¸€æ­¥éƒ½åˆ©ç”¨ä¹‹å‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆè§£å†³é˜¶æ®µï¼‰ã€‚
    3.  ç»´æŠ¤æ‰§è¡Œå†å²å’Œä¸Šä¸‹æ–‡ã€‚
    4.  å¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚
    è¿™ç§æ¨¡å¼ç‰¹åˆ«é€‚åˆéœ€è¦å¤šæ­¥æ¨ç†ã€æ•°å­¦è®¡ç®—ã€å¤æ‚åˆ†æç­‰ä»»åŠ¡ã€‚
    """
    def __init__(
        self,
        name: str,
        llm_client: HelloAgentsLLM,
        system_prompt: Optional[str] = None,
        config: Optional[Config] = None,
        custom_prompts: Optional[Dict[str, str]] = None
    ):
        """
        åˆå§‹åŒ– Plan and Solve Agentã€‚

        Args:
            name (str): Agentçš„åç§°ã€‚
            llm_client (HelloAgentsLLM): LLMå®¢æˆ·ç«¯å®ä¾‹ã€‚
            system_prompt (Optional[str]): ç³»ç»Ÿçš„é¡¶çº§æç¤ºè¯ï¼ˆå¦‚æœéœ€è¦ï¼‰ã€‚
            config (Optional[Config]): é…ç½®å¯¹è±¡ã€‚
            custom_prompts (Optional[Dict[str, str]]): ä¸€ä¸ªåŒ…å«è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿çš„å­—å…¸ï¼Œé”®ä¸º "planner" å’Œ "executor"ã€‚
        """
        # è°ƒç”¨çˆ¶ç±»Agentçš„æ„é€ å‡½æ•°è¿›è¡ŒåŸºæœ¬åˆå§‹åŒ–
        super().__init__(name, llm_client, system_prompt, config)

        # è®¾ç½®è§„åˆ’å™¨å’Œæ‰§è¡Œå™¨çš„æç¤ºè¯æ¨¡æ¿ï¼šä¼˜å…ˆä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰çš„ï¼Œå¦åˆ™ä¸ºNoneï¼ˆå°†è§¦å‘ä½¿ç”¨é»˜è®¤æ¨¡æ¿ï¼‰
        if custom_prompts:
            planner_prompt = custom_prompts.get("planner")
            executor_prompt = custom_prompts.get("executor")
        else:
            planner_prompt = None
            executor_prompt = None

        # å®ä¾‹åŒ–è§„åˆ’å™¨ç»„ä»¶
        self.planner = Planner(llm_client, planner_prompt)
        # å®ä¾‹åŒ–æ‰§è¡Œå™¨ç»„ä»¶
        self.executor = Executor(llm_client, executor_prompt)

    def run(self, input_text: str, **kwargs) -> str:
        """
        è¿è¡Œ Plan and Solve Agent çš„ä¸»æµç¨‹ã€‚

        Args:
            input_text (str): ç”¨æˆ·è¾“å…¥çš„éœ€è¦è§£å†³çš„é—®é¢˜ã€‚
            **kwargs: ä¼ é€’ç»™LLMè°ƒç”¨çš„é¢å¤–å‚æ•°ã€‚

        Returns:
            str: é—®é¢˜çš„æœ€ç»ˆç­”æ¡ˆã€‚
        """
        print(f"\nğŸ¤–:{self.name}å¼€å§‹å¤„ç†é—®é¢˜{input_text}")

        # --- é˜¶æ®µ1: ç”Ÿæˆè®¡åˆ’ ---
        plan = self.planner.plan(input_text, **kwargs)
        # æ£€æŸ¥è®¡åˆ’æ˜¯å¦æˆåŠŸç”Ÿæˆ
        if not plan:
            # å¦‚æœè®¡åˆ’åˆ—è¡¨ä¸ºç©ºï¼Œè¯´æ˜è§„åˆ’å¤±è´¥ï¼Œä»»åŠ¡æ— æ³•ç»§ç»­
            final_answer = "æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„è¡ŒåŠ¨è®¡åˆ’ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚"
            print(f"\n --- ä»»åŠ¡ä¸­æ­¢ ---\n{final_answer}")

            # å°†æ­¤æ¬¡å¤±è´¥çš„äº¤äº’è®°å½•åˆ°å†å²æ¶ˆæ¯ä¸­
            self.add_message(Message(input_text, "user"))
            self.add_message(Message(final_answer, "assistant"))

            return final_answer

        # --- é˜¶æ®µ2: æ‰§è¡Œè®¡åˆ’ ---
        final_answer = self.executor.execute(input_text, plan, **kwargs)
        print(f"\n --- ä»»åŠ¡å®Œæˆ ---\næœ€ç»ˆç­”æ¡ˆ:{final_answer}")

        # å°†æˆåŠŸçš„äº¤äº’ï¼ˆç”¨æˆ·é—®é¢˜å’Œæœ€ç»ˆç­”æ¡ˆï¼‰è®°å½•åˆ°å†å²æ¶ˆæ¯ä¸­
        self.add_message(Message(input_text, "user"))
        self.add_message(Message(final_answer, "assistant"))

        # è¿”å›æœ€ç»ˆç­”æ¡ˆ
        return final_answer
